---
title: "Binary Logit Model"
author: "Lunjing Yuan"
date: "Monday, 2/8/2021"
output:
  word_document: default
  html_document: default
  pdf_document: default
graphics: yes
urlcolor: blue
---

Read in the SCU reunion data
```{r}
rm(list=ls())       # Remove anything that might currently be in memory so we can start fresh.
library(data.table) # Load the data.table package.
library(MASS)       # Load the MASS package

reuniondata <- fread('Reuniondata_inclass.csv')
```

You can check out the variables names using
```{r}
names(reuniondata)
```
The interpretation of each variable is in the file Reuniondata_inclass.xlsx, in a separate sheet. Please check that sheet to understand the meaning of each variable. 

Here, if you prefer using data.frame instead of data.table, that is fine as well.

We first created the $Y$ variable, based on the variable "donatesum", indicating the "total number of donations". As our first order interest is whether someone donate or not, rather than how much they donate, we decided to create a choice variable, as $Y$ for the binary logit model, and added the variable to the last column in the `reuniondata` datatable. 

```{r}
reuniondata[,Choice:=as.numeric(donatesum>0),]
dim(reuniondata)
```

As a starter, we estimate the binary logit model using all the other variables as $X$ variable. To do that, I first created a data frame, that contains the choice variable, and all the $X$ variables. 

```{r }
d = data.frame(y=reuniondata[,17],reuniondata[,4:16])
```

Before using the data, I first split the data into training set with the first 3000 data points and testing data with the rest.  

```{r }
idTrn = 1:3000  # row index for the training data
idTst = !(1:nrow(reuniondata) %in% idTrn)  # row index for the testing data

blTrn_basic = glm(Choice~ ., data=d, family="binomial", subset = idTrn) 
#using only training data. Note that the "." after "~" meaning using all the rest of the data in data frame "d" except the variable "Choice". 
#This is why we created the "d" dataframe, so that we do not need to list all the other variables we need in the model. 
#but you can always use the following command, if you do not want to use "."

#blTrn_basic = glm(Choice~ SpouseAlum+SportsAlum+UGAlumAwards+OtherUGAct+EverAssigned+BoardMember+
#ChildAlum+ParentAlum+SiblingAlum+GradDegree+TotalReunions+OnePlusEvents+
#Years.Lapsed, data=d, family="binomial", subset = idTrn)  
#for running additional models, you may find it easier to base your code on this code. 

summary(blTrn_basic)
```

> Q1: Among these results, the variable with the highest t-value is Years.Lapsed, indicating the "number of years elapsed since last donation". Its t-value has an absolute value that is much higher than the other t-values. That is usually a sign of a problem. What could be the problem here? Think about the model estimation is trying to match the data (values in Y).

## The Years.Lapsed variable has a much higher absolute t-value compared to the other variables. This is likely because most independent variables are binary, taking on values of just 0 or 1. However, Years.Lapsed represents the total number of years since the alumnus/alumna last donated to SCU, with 48 indicating no prior donation history. The continuous, numeric nature of this variable, with a wide range of values, contributes to its high t-value. More importantly, including Years.Lapsed could negatively impact the model in two key ways. First, it could distort the overall p-value for the regression, making it seem more statistically significant than it should be. Second, it could obscure the true relationships between valid, useful predictor variables and the response. Problematic variables like Years.Lapsed can impair the regression both mathematically and interpretively. Careful examination of this variable's role is warranted.

## In summary, the exceptionally high t-value for Years.Lapsed stems from its continuous nature and wide range of values. Its inclusion risks reducing the validity and interpretability of the regression results. Assessing whether it should be retained requires further analysis.


> Q2: Now let's re-estimate the model using some different specifications that you can come up with. Estimate at least 2 more models. I have couple of suggestions:   

- Alternative model 1: drop the varaible "Years.Lapsed"   

```{r}

d1 = data.frame(y=reuniondata[,17],reuniondata[,4:15])

blTrn_basic_dropY = glm(Choice~ 
SpouseAlum+SportsAlum+UGAlumAwards+OtherUGAct+EverAssigned+BoardMember
+ChildAlum+ParentAlum+SiblingAlum+GradDegree+TotalReunions+OnePlusEvents, data=d1, family="binomial", subset = idTrn)

summary(blTrn_basic_dropY)
```   

- Alternative model 2: You can consider combining some of the variables to create a new dummy variable, for example

```{r}
famAlumdata = reuniondata[,.(SpouseAlum,ChildAlum,ParentAlum,SiblingAlum)]
FamilyAlum = as.numeric(rowSums(famAlumdata)>0)

reuniondata[,FamilyAlum:=as.numeric(rowSums(famAlumdata)>0),]
d2 = data.frame(y=reuniondata[,17],reuniondata[,4:18])


blTrn_basic_combine = glm(Choice~
SportsAlum+UGAlumAwards+OtherUGAct+EverAssigned+BoardMember+GradDegree+TotalReunions
+OnePlusEvents+Years.Lapsed+FamilyAlum, data=d2, family="binomial", subset = idTrn)

summary(blTrn_basic_combine)
```   

- Alternative model 3: Based on the variable `RYCohort`, you can create a variable to calculate the number of years since graduation, then add this variable to the model, and see whether the estimation results improve. 
When estimating these other models, use the second `glm()` function, and then adding or dropping variables

```{r}
yearsgrad = 2014-reuniondata[,RYCohort]
reuniondata[,yearsgrad:=2014-reuniondata[,RYCohort],]
d3 = data.frame(y=reuniondata[,17],reuniondata[,4:19])


blTrn_basic_yearsgrad = glm(Choice~ yearsgrad+SpouseAlum+SportsAlum
+UGAlumAwards+OtherUGAct+EverAssigned+BoardMember+ChildAlum+ParentAlum+SiblingAlum+GradDegree
+TotalReunions+OnePlusEvents+Years.Lapsed, data=d3, family="binomial", subset = idTrn)

summary(blTrn_basic_yearsgrad)
```


> Q3: Now let's try out-of-sample test on the testing data set. Using each of the three models you estimated above, calculate the following metrics for each model. 

* ln-likelihood value on the testing data.
* Plot the ROC and calculate the AUC values, using the data from the lecture note. 

> To do that for your basic model for ln-liklihood, you can use the following code

```{r}
yActual = d[idTst,1] #get the actual value for the choice variable
predTst_basic = predict(blTrn_basic, d[idTst,], type="response") 
#use the model results in blTrn_basic, to predict the probability of Y=1 for each data point in the testing data set

lnlike_basic = sum(log(predTst_basic*yActual+(1-predTst_basic)*(1-yActual))) 
#using the predicted probability that Y=1, and the actual data, can calculate the ln-likelihood for all the data points
lnlike_basic


# Plot the ROC and calculate the AUC values for basic model. 
# install.packages("ROCR")
library(ROCR)
pred <- prediction(predTst_basic,yActual) 
perf <- performance(pred,"tpr","fpr") 
plot(perf,col='blue')

#  AUC value
perf <- performance(pred,measure="auc") 
print(paste("AUC= ", perf@y.values[[1]]))

```

> Repeat the above code for your two additional models, and compare the results and conclude which model is best for the out-of-sample test? Comment on the parts that you do not like about the chosen model.

```{r}
# Alternative model 1

yActual = d[idTst,1] #get the actual value for the choice variable
predTst_basic_a1 = predict(blTrn_basic_dropY, d1[idTst,], type="response") 
#use the model results in blTrn_basic, to predict the probability of Y=1 for each data point in the testing data set

lnlike_basic_a1 = sum(log(predTst_basic_a1*yActual+(1-predTst_basic_a1)*(1-yActual))) 
#using the predicted probability that Y=1, and the actual data, can calculate the ln-likelihood for all the data points
lnlike_basic_a1


# Plot the ROC and calculate the AUC values for model 1. 
# install.packages("ROCR")
library(ROCR)
pred1 <- prediction(predTst_basic_a1,yActual) 
perf1 <- performance(pred1,"tpr","fpr") 
plot(perf1,col='blue')

# AUC value for model 1
perf1 <- performance(pred1,measure="auc") 
print(paste("AUC_m1= ", perf1@y.values[[1]]))

```

```{r}
# Alternative model 2

yActual = d[idTst,1] #get the actual value for the choice variable
predTst_basic_a2 = predict(blTrn_basic_combine, d2[idTst,], type="response") 
#use the model results in blTrn_basic, to predict the probability of Y=1 for each data point in the testing data set

lnlike_basic_a2 = sum(log(predTst_basic_a2*yActual+(1-predTst_basic_a2)*(1-yActual))) 
#using the predicted probability that Y=1, and the actual data, can calculate the ln-likelihood for all the data points
lnlike_basic_a2

# Plot the ROC and calculate the AUC values for model 2. 
# install.packages("ROCR")
library(ROCR)
pred2 <- prediction(predTst_basic_a2,yActual) 
perf2 <- performance(pred2,"tpr","fpr") 
plot(perf2,col='blue')

# AUC value for model 2
perf2 <- performance(pred2,measure="auc") 
print(paste("AUC_m2= ", perf2@y.values[[1]]))
```

```{r}
# Alternative model 3

yActual = d[idTst,1] #get the actual value for the choice variable
predTst_basic_a3 = predict(blTrn_basic_yearsgrad, d3[idTst,], type="response") 
#use the model results in blTrn_basic, to predict the probability of Y=1 for each data point in the testing data set

lnlike_basic_a3 = sum(log(predTst_basic_a3*yActual+(1-predTst_basic_a3)*(1-yActual))) 
#using the predicted probability that Y=1, and the actual data, can calculate the ln-likelihood for all the data points
lnlike_basic_a3


# Plot the ROC and calculate the AUC values for model 3. 
# install.packages("ROCR")
library(ROCR)
pred3 <- prediction(predTst_basic_a3,yActual) 
perf3 <- performance(pred3,"tpr","fpr") 
plot(perf3,col='blue')

# AUC value for model 3
perf3 <- performance(pred3,measure="auc") 
print(paste("AUC_3= ", perf3@y.values[[1]]))
```

#######
#which model is best for the out-of-sample test? Comment on the parts that you do not like about the chosen model.


## (1) Our goal is to maximize the log-likelihood, where higher values indicate a better model fit. Among the models tested, Model 2 has the highest log-likelihood value of -1403.629, making it the best fitting model for the data.

## (2) A limitation of Model 2 is that it combines the four previous alumni variables (SpouseAlum, ChildAlum, ParentAlum, SiblingAlum) into one new dummy variable called FamilyAlum. While this improved model fit, it also reduced the specificity of information we have about each type of alumnus relationship. By consolidating these variables, we lose granular insights into how having a spouse, child, parent or sibling as an alumnus may differently influence donation likelihood. For certain analytical needs, retaining the separated variables could provide useful details that the combined FamilyAlum variable masks.

## In summary, Model 2 improves overall fit but reduces specificity in exchange. This illustrates a common tradeoff in modeling between parsimony and insight. Further analysis of variable importance could help determine if the detailed alumni variables should be retained moving forward.
